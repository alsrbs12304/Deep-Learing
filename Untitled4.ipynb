{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqrP+yO14fTgfdvYMCiJsX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alsrbs12304/Deep-Learing/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X76dRa4zDPyD"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KTQYU5jDTfX",
        "outputId": "bc88754b-ef3f-42cd-cab4-4d03478e0005"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 1.9.0+cu111  Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3YKgKDWDViX"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUvfV6atDZtT",
        "outputId": "872aef0b-3b30-4a0a-9f65-550aed87566d"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PIOu2T3Dapy",
        "outputId": "24e9b8ab-7a77-4287-a231-1be85974bb64"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8go_UQ-DdNv",
        "outputId": "2e3f116c-aa37-4ccc-d6da-89f1879ece87"
      },
      "source": [
        "train_dataset = datasets.MNIST(root = \"./data/MNIST\",\n",
        "                               train = True,\n",
        "                               download = True,\n",
        "                               transform = transforms.ToTensor())\n",
        "\n",
        "test_dataset = datasets.MNIST(root = \"./data/MNIST\",\n",
        "                              train = False,\n",
        "                              transform = transforms.ToTensor())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6cLeDVZDfQN"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = BATCH_SIZE,\n",
        "                                           shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size = BATCH_SIZE,\n",
        "                                          shuffle = False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wpX4jt1Dgas",
        "outputId": "7037ef36-1c45-4c43-d8e7-40d8acf2ab2b"
      },
      "source": [
        "for (X_train, y_train) in train_loader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
            "y_train: torch.Size([32]) type: torch.LongTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "x8EuVVpyDhsQ",
        "outputId": "93595409-b7f1-4395-af82-0caf04f5d1a3"
      },
      "source": [
        "pltsize = 1\n",
        "plt.figure(figsize=(10 * pltsize, pltsize))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
        "    plt.title('Class: ' + str(y_train[i].item()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zedtWBdl5O6M"
      },
      "source": [
        "# Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ipt9oMODjRZ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxhCnO01Dl59",
        "outputId": "de33b2e9-9e1b-4de7-f53c-8c0cfc29b400"
      },
      "source": [
        "model = Net().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHx1Jg4nEVce"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(image), \n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
        "                loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-IeuMTnEWEY"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim = True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "    \n",
        "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prvrs3R1EXbS",
        "outputId": "f9df27f8-3c32-4bd1-b1f0-a6f8178e43c5"
      },
      "source": [
        "#@title\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval = 200)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
        "        epoch, test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.434333\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.414395\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.277908\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.353316\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.542946\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.625217\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.342085\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.314643\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.160924\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.362432\n",
            "\n",
            "[EPOCH: 1], \tTest Loss: 0.1492, \tTest Accuracy: 95.60 % \n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.171501\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.228426\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.272895\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.152473\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.163313\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.064482\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.716168\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.531752\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.317965\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.105608\n",
            "\n",
            "[EPOCH: 2], \tTest Loss: 0.1120, \tTest Accuracy: 96.62 % \n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.398250\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.071356\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.225731\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.222354\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.299156\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.154108\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.118882\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.161161\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.086886\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.085481\n",
            "\n",
            "[EPOCH: 3], \tTest Loss: 0.0970, \tTest Accuracy: 96.90 % \n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.259391\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.220535\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.084835\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.224616\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.110541\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.288306\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.068254\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.120326\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.468463\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.148897\n",
            "\n",
            "[EPOCH: 4], \tTest Loss: 0.0859, \tTest Accuracy: 97.37 % \n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.102085\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.168475\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.141379\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.118286\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.073006\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.163958\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.218148\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.390795\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.476599\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.059526\n",
            "\n",
            "[EPOCH: 5], \tTest Loss: 0.0799, \tTest Accuracy: 97.61 % \n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.119065\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.138967\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.087090\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.078568\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.092914\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.077961\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.131087\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.088443\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.141716\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.062066\n",
            "\n",
            "[EPOCH: 6], \tTest Loss: 0.0772, \tTest Accuracy: 97.62 % \n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.078769\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.042452\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.060046\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.072549\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.038254\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.205339\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.121607\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.203731\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.131978\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.163268\n",
            "\n",
            "[EPOCH: 7], \tTest Loss: 0.0723, \tTest Accuracy: 97.77 % \n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.073366\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.038600\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.253343\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.088564\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.442423\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.418149\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.134290\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.172049\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.452969\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.125385\n",
            "\n",
            "[EPOCH: 8], \tTest Loss: 0.0686, \tTest Accuracy: 97.92 % \n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.156405\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.204805\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.244407\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.042981\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.062392\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.214735\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.026922\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.240605\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.211976\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.182050\n",
            "\n",
            "[EPOCH: 9], \tTest Loss: 0.0672, \tTest Accuracy: 97.81 % \n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.165349\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.028427\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.133490\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.044778\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.142183\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.118831\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.048538\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.220887\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.137338\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.198886\n",
            "\n",
            "[EPOCH: 10], \tTest Loss: 0.0632, \tTest Accuracy: 98.05 % \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aaetp0ex6HKx"
      },
      "source": [
        "# with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMVG6JKX6HK6"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout_prob = 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc2(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y05xWWDe6HK6",
        "outputId": "de33b2e9-9e1b-4de7-f53c-8c0cfc29b400"
      },
      "source": [
        "model = Net().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pR5cdNl6HK7"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(image), \n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
        "                loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_SX8yRk6HK7"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim = True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "    \n",
        "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2qt390K6HK7",
        "outputId": "f9df27f8-3c32-4bd1-b1f0-a6f8178e43c5"
      },
      "source": [
        "#@title\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval = 200)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
        "        epoch, test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.434333\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.414395\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.277908\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.353316\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.542946\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.625217\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.342085\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.314643\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.160924\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.362432\n",
            "\n",
            "[EPOCH: 1], \tTest Loss: 0.1492, \tTest Accuracy: 95.60 % \n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.171501\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.228426\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.272895\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.152473\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.163313\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.064482\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.716168\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.531752\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.317965\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.105608\n",
            "\n",
            "[EPOCH: 2], \tTest Loss: 0.1120, \tTest Accuracy: 96.62 % \n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.398250\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.071356\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.225731\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.222354\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.299156\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.154108\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.118882\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.161161\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.086886\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.085481\n",
            "\n",
            "[EPOCH: 3], \tTest Loss: 0.0970, \tTest Accuracy: 96.90 % \n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.259391\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.220535\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.084835\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.224616\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.110541\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.288306\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.068254\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.120326\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.468463\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.148897\n",
            "\n",
            "[EPOCH: 4], \tTest Loss: 0.0859, \tTest Accuracy: 97.37 % \n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.102085\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.168475\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.141379\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.118286\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.073006\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.163958\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.218148\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.390795\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.476599\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.059526\n",
            "\n",
            "[EPOCH: 5], \tTest Loss: 0.0799, \tTest Accuracy: 97.61 % \n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.119065\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.138967\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.087090\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.078568\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.092914\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.077961\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.131087\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.088443\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.141716\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.062066\n",
            "\n",
            "[EPOCH: 6], \tTest Loss: 0.0772, \tTest Accuracy: 97.62 % \n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.078769\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.042452\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.060046\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.072549\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.038254\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.205339\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.121607\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.203731\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.131978\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.163268\n",
            "\n",
            "[EPOCH: 7], \tTest Loss: 0.0723, \tTest Accuracy: 97.77 % \n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.073366\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.038600\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.253343\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.088564\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.442423\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.418149\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.134290\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.172049\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.452969\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.125385\n",
            "\n",
            "[EPOCH: 8], \tTest Loss: 0.0686, \tTest Accuracy: 97.92 % \n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.156405\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.204805\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.244407\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.042981\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.062392\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.214735\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.026922\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.240605\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.211976\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.182050\n",
            "\n",
            "[EPOCH: 9], \tTest Loss: 0.0672, \tTest Accuracy: 97.81 % \n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.165349\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.028427\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.133490\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.044778\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.142183\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.118831\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.048538\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.220887\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.137338\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.198886\n",
            "\n",
            "[EPOCH: 10], \tTest Loss: 0.0632, \tTest Accuracy: 98.05 % \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io2DbdQb6eaJ"
      },
      "source": [
        "# with Dropout + ReLu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgi0XzWS6eaS"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout_prob = 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cK0lPF66eaT",
        "outputId": "8016b9fb-d571-4be5-f6b2-41ec790321f2"
      },
      "source": [
        "model = Net().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW5p9Af36eaT"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(image), \n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
        "                loss.item()))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD0qr0UQ6eaT"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim = True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "    \n",
        "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "GuBvWH3i6eaT",
        "outputId": "100c20b5-d9c2-49a6-a33d-1c21e8ebd5ae"
      },
      "source": [
        "#@title\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval = 200)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
        "        epoch, test_loss, test_accuracy))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.328500\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.044343\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ef240ef05655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
            "\u001b[0;32m<ipython-input-34-be98d565d6de>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, log_interval)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlYt9axU6vP5"
      },
      "source": [
        "# with Dropout + ReLu + BN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urRHr5mb6vQB"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout_prob = 0.5\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7plJmT36vQC",
        "outputId": "75a92391-51e5-454d-dc36-023152db7aab"
      },
      "source": [
        "model = Net().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcvCL5fM6vQC"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(image), \n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
        "                loss.item()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d32uVNHh6vQC"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim = True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "    \n",
        "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XC-YF7F6vQC",
        "outputId": "88affaf4-7191-4adb-825f-b8687457cc9d"
      },
      "source": [
        "#@title\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval = 200)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
        "        epoch, test_loss, test_accuracy))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.156394\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.473387\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.401485\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.599920\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.562107\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.418843\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.134657\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.244492\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.338070\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.423866\n",
            "\n",
            "[EPOCH: 1], \tTest Loss: 0.1528, \tTest Accuracy: 95.36 % \n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.150224\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.185134\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.512642\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.216087\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.358434\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.316974\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.276730\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.222777\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.453492\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.329371\n",
            "\n",
            "[EPOCH: 2], \tTest Loss: 0.1152, \tTest Accuracy: 96.44 % \n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.067687\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.126027\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.137018\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.181112\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.072788\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.017463\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.159955\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.201971\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.301329\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.110011\n",
            "\n",
            "[EPOCH: 3], \tTest Loss: 0.0960, \tTest Accuracy: 96.98 % \n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.225831\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.219085\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.107300\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.256421\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.085986\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.121963\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.048826\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.180026\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.202663\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.080787\n",
            "\n",
            "[EPOCH: 4], \tTest Loss: 0.0860, \tTest Accuracy: 97.24 % \n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.249306\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.122233\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.044444\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.121562\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.055313\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.138908\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.179644\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.182150\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.266652\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.042528\n",
            "\n",
            "[EPOCH: 5], \tTest Loss: 0.0825, \tTest Accuracy: 97.51 % \n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.095590\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.060809\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.085036\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.060877\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.371727\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.051134\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.095055\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.077974\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.091122\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.108170\n",
            "\n",
            "[EPOCH: 6], \tTest Loss: 0.0756, \tTest Accuracy: 97.70 % \n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.278063\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.086651\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.046364\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.026815\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.030980\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.081339\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.180907\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.138341\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.462352\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.269842\n",
            "\n",
            "[EPOCH: 7], \tTest Loss: 0.0695, \tTest Accuracy: 97.79 % \n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.432051\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.317082\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.131527\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.051730\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.385593\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.085756\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.090439\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.059655\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.198155\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.130809\n",
            "\n",
            "[EPOCH: 8], \tTest Loss: 0.0687, \tTest Accuracy: 97.97 % \n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.097269\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.012388\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.097160\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.181850\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.269486\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.178913\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.209741\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.101946\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.210591\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.082274\n",
            "\n",
            "[EPOCH: 9], \tTest Loss: 0.0707, \tTest Accuracy: 97.82 % \n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.103778\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.080663\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.057741\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.085216\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.108206\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.131859\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.062804\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.028275\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.019557\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.033441\n",
            "\n",
            "[EPOCH: 10], \tTest Loss: 0.0623, \tTest Accuracy: 98.00 % \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4YReVSr7hp_"
      },
      "source": [
        "# with Dropout + ReLu + BN + He\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOmg6vEK7hqI"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout_prob = 0.5\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6VlFUAr7hqI",
        "outputId": "098e9bae-6878-4283-8738-1a4e842691f1"
      },
      "source": [
        "import torch.nn.init as init\n",
        "def weight_init(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    init.kaiming_uniform_(m.weight.data)\n",
        "\n",
        "model = Net().to(DEVICE)\n",
        "model.apply(weight_init)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N4cWJjU7hqI"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(image), \n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
        "                loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilbn1s8T7hqJ"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim = True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "    \n",
        "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3TuECDv7hqJ",
        "outputId": "f9df27f8-3c32-4bd1-b1f0-a6f8178e43c5"
      },
      "source": [
        "#@title\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval = 200)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
        "        epoch, test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.434333\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.414395\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.277908\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.353316\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.542946\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.625217\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.342085\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.314643\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.160924\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.362432\n",
            "\n",
            "[EPOCH: 1], \tTest Loss: 0.1492, \tTest Accuracy: 95.60 % \n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.171501\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.228426\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.272895\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.152473\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.163313\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.064482\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.716168\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.531752\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.317965\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.105608\n",
            "\n",
            "[EPOCH: 2], \tTest Loss: 0.1120, \tTest Accuracy: 96.62 % \n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.398250\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.071356\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.225731\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.222354\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.299156\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.154108\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.118882\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.161161\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.086886\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.085481\n",
            "\n",
            "[EPOCH: 3], \tTest Loss: 0.0970, \tTest Accuracy: 96.90 % \n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.259391\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.220535\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.084835\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.224616\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.110541\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.288306\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.068254\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.120326\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.468463\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.148897\n",
            "\n",
            "[EPOCH: 4], \tTest Loss: 0.0859, \tTest Accuracy: 97.37 % \n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.102085\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.168475\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.141379\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.118286\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.073006\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.163958\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.218148\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.390795\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.476599\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.059526\n",
            "\n",
            "[EPOCH: 5], \tTest Loss: 0.0799, \tTest Accuracy: 97.61 % \n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.119065\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.138967\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.087090\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.078568\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.092914\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.077961\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.131087\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.088443\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.141716\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.062066\n",
            "\n",
            "[EPOCH: 6], \tTest Loss: 0.0772, \tTest Accuracy: 97.62 % \n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.078769\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.042452\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.060046\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.072549\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.038254\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.205339\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.121607\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.203731\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.131978\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.163268\n",
            "\n",
            "[EPOCH: 7], \tTest Loss: 0.0723, \tTest Accuracy: 97.77 % \n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.073366\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.038600\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.253343\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.088564\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.442423\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.418149\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.134290\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.172049\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.452969\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.125385\n",
            "\n",
            "[EPOCH: 8], \tTest Loss: 0.0686, \tTest Accuracy: 97.92 % \n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.156405\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.204805\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.244407\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.042981\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.062392\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.214735\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.026922\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.240605\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.211976\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.182050\n",
            "\n",
            "[EPOCH: 9], \tTest Loss: 0.0672, \tTest Accuracy: 97.81 % \n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.165349\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.028427\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.133490\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.044778\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.142183\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.118831\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.048538\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.220887\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.137338\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.198886\n",
            "\n",
            "[EPOCH: 10], \tTest Loss: 0.0632, \tTest Accuracy: 98.05 % \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmEP1rMf8RMf"
      },
      "source": [
        "# with Dropout + ReLu + BN + He + Adam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOn_xejI8RMn"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout_prob = 0.5\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEbt0hdC8RMo",
        "outputId": "0fbb1bd8-9404-47b4-dbc2-8e6c9de52b1a"
      },
      "source": [
        "import torch.nn.init as init\n",
        "def weight_init(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    init.kaiming_uniform_(m.weight.data)\n",
        "\n",
        "model = Net().to(DEVICE)\n",
        "model.apply(weight_init)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS9S-Jso8RMo"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(image), \n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
        "                loss.item()))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edraGPEI8RMo"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim = True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "    \n",
        "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "JqpT2fft8RMo",
        "outputId": "1a99c103-fc9d-42d1-d8e7-be56f93fb278"
      },
      "source": [
        "#@title\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval = 200)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
        "        epoch, test_loss, test_accuracy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 3.226567\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.782942\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.449805\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.423105\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.883316\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.213023\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.321610\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.375515\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.393329\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.775837\n",
            "\n",
            "[EPOCH: 1], \tTest Loss: 0.1440, \tTest Accuracy: 95.75 % \n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.483084\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.207655\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.111262\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.320957\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.400236\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.233719\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.162042\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.256379\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.169991\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.151154\n",
            "\n",
            "[EPOCH: 2], \tTest Loss: 0.1027, \tTest Accuracy: 96.85 % \n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.212251\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.446084\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.174785\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.051395\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.116184\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.096921\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.353870\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.244036\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.213937\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.224436\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ef240ef05655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
            "\u001b[0;32m<ipython-input-19-be98d565d6de>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, log_interval)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2670\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2671\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2569\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2571\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m         \u001b[0;31m# RGB or RGBA value for a P image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0m_close_exclusive_fp_after_loading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;31m# FIXME: take \"new\" parameters / other image?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# FIXME: turn mode and size into delegating properties?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}